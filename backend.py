# -*- coding: utf-8 -*-
"""Copy of Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MheZQE-JoTWr6T-t0VTXJ8tf-Rl4dqvO
"""

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import librosa.display

# Directory path to dataset (replace with the actual path to your dataset folder)
dataset_dir = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data'

# Function to load all audio files and extract their corresponding labels
def load_dataset(directory):
    features = []
    mel_spectrograms = []
    labels = []

    # Assuming each subfolder in 'directory' is a class label, with audio files inside
    for class_label in os.listdir(directory):
        class_dir = os.path.join(directory, class_label)
        if os.path.isdir(class_dir):
            for file_name in os.listdir(class_dir):
                file_path = os.path.join(class_dir, file_name)
                if file_path.endswith('.wav'):
                    feat, mel = extract_features(file_path)
                    features.append(feat)
                    mel_spectrograms.append(mel)
                    labels.append(class_label)
    return np.array(features), np.array(labels), mel_spectrograms

# Feature extraction function
def extract_features(file_path, sr=22050):
    # Load audio file
    y, sr = librosa.load(file_path, sr=sr, duration=5.0)

    # Time domain features
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))
    rms = np.mean(librosa.feature.rms(y=y))

    # Frequency domain features (MFCCs)
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)

    # Spectrogram
    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)
    mel_db = librosa.power_to_db(mel_spectrogram, ref=np.max)

    return [zcr, rms] + mfccs.tolist(), mel_db

# Load dataset
X, y, mel_spectrograms = load_dataset(dataset_dir)

# Convert labels to numerical values if needed
label_map = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_map[label] for label in y])

# Convert features to DataFrame
columns = ['ZCR', 'RMS'] + [f'MFCC_{i}' for i in range(20)]
df = pd.DataFrame(X, columns=columns)
df['label'] = y

# Visualize Mel-Spectrogram for the first audio file
plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spectrograms[0], sr=22050, x_axis='time', y_axis='mel')
plt.title('Mel-Spectrogram (Sample 1)')
plt.colorbar(format='%+2.0f dB')
plt.show()

# Split the dataset into training and testing sets
X = df.drop('label', axis=1)
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()

# Initialize models
rf = RandomForestClassifier(random_state=42)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Hyperparameter tuning with GridSearchCV for Random Forest
param_grid_rf = {
    'rf__n_estimators': [100, 200],
    'rf__max_depth': [10, 20, None], # Changed from 'max_depth' to 'rf__max_depth'
    'rf__min_samples_split': [2, 5], # Changed from 'min_samples_split' to 'rf__min_samples_split'
    'rf__min_samples_leaf': [1, 2], # Changed from 'min_samples_leaf' to 'rf__min_samples_leaf'
}
# Create pipeline
pipe_rf = Pipeline([('scaler', scaler), ('rf', rf)])
grid_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=5, verbose=1, n_jobs=-1)
grid_rf.fit(X_train, y_train)

# Train XGBoost model
pipe_xgb = Pipeline([('scaler', scaler), ('xgb', xgb)])
pipe_xgb.fit(X_train, y_train)

# Predictions
y_pred_rf = grid_rf.predict(X_test)
y_pred_xgb = pipe_xgb.predict(X_test)

# Evaluation - Random Forest
train_acc_rf = grid_rf.score(X_train, y_train)
test_acc_rf = grid_rf.score(X_test, y_test)
print(f"Random Forest - Training Accuracy: {train_acc_rf:.2f}")
print(f"Random Forest - Testing Accuracy: {test_acc_rf:.2f}")

# Evaluation - XGBoost
train_acc_xgb = pipe_xgb.score(X_train, y_train)
test_acc_xgb = pipe_xgb.score(X_test, y_test)
print(f"XGBoost - Training Accuracy: {train_acc_xgb:.2f}")
print(f"XGBoost - Testing Accuracy: {test_acc_xgb:.2f}")

# Detailed Classification Report
print("\nRandom Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

print("\nXGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

# Confusion Matrices
cm_rf = confusion_matrix(y_test, y_pred_rf)
cm_xgb = confusion_matrix(y_test, y_pred_xgb)

# Plot confusion matrix for Random Forest
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest Confusion Matrix')
plt.show()

# Plot confusion matrix for XGBoost
plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues')
plt.title('XGBoost Confusion Matrix')
plt.show()

# Feature Importance (Random Forest)
importances_rf = grid_rf.best_estimator_.named_steps['rf'].feature_importances_
indices_rf = np.argsort(importances_rf)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance - Random Forest")
plt.bar(range(X.shape[1]), importances_rf[indices_rf], align="center")
plt.xticks(range(X.shape[1]), X.columns[indices_rf], rotation=90)
plt.tight_layout()
plt.show()

# Feature Importance (XGBoost)
importances_xgb = pipe_xgb.named_steps['xgb'].feature_importances_
indices_xgb = np.argsort(importances_xgb)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance - XGBoost")
plt.bar(range(X.shape[1]), importances_xgb[indices_xgb], align="center")
plt.xticks(range(X.shape[1]), X.columns[indices_xgb], rotation=90)
plt.tight_layout()
plt.show()



# Hyperparameter tuning with GridSearchCV for Random Forest
param_grid_rf = {
    'rf__n_estimators': [200, 300, 400],  # Increased number of trees
    'rf__max_depth': [20, 30, None],      # Increased depth
    'rf__min_samples_split': [2, 5, 10],  # Tuning minimum samples split
    'rf__min_samples_leaf': [1, 2, 4]     # Tuning minimum samples leaf
}

# Create a pipeline with Random Forest and standard scaler
pipe_rf = Pipeline([('scaler', scaler), ('rf', rf)])
grid_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=5, verbose=1, n_jobs=-1)

# Fit the model with GridSearchCV
grid_rf.fit(X_train, y_train)

# Train XGBoost model with hyperparameter tuning
param_grid_xgb = {
    'xgb__n_estimators': [200, 300],            # Increased number of trees
    'xgb__learning_rate': [0.01, 0.1],          # Adjust learning rate
    'xgb__max_depth': [6, 10],                  # Increased depth
    'xgb__subsample': [0.8, 1.0],               # Use subsampling for regularization
    'xgb__colsample_bytree': [0.8, 1.0]         # Column sampling for regularization
}

# Create a pipeline with XGBoost and standard scaler
pipe_xgb = Pipeline([('scaler', scaler), ('xgb', xgb)])
grid_xgb = GridSearchCV(pipe_xgb, param_grid_xgb, cv=5, verbose=1, n_jobs=-1)

# Fit the model with GridSearchCV
grid_xgb.fit(X_train, y_train)

# Predictions after tuning
y_pred_rf = grid_rf.predict(X_test)
y_pred_xgb = grid_xgb.predict(X_test)

# Evaluation after tuning - Random Forest
train_acc_rf = grid_rf.score(X_train, y_train)
test_acc_rf = grid_rf.score(X_test, y_test)
print(f"Random Forest - Training Accuracy: {train_acc_rf:.2f}")
print(f"Random Forest - Testing Accuracy: {test_acc_rf:.2f}")

# Evaluation after tuning - XGBoost
train_acc_xgb = grid_xgb.score(X_train, y_train)
test_acc_xgb = grid_xgb.score(X_test, y_test)
print(f"XGBoost - Training Accuracy: {train_acc_xgb:.2f}")
print(f"XGBoost - Testing Accuracy: {test_acc_xgb:.2f}")

# Detailed Classification Report for both models
print("\nRandom Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

print("\nXGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

# Plot training and testing accuracy graph
model_names = ['Random Forest', 'XGBoost']
training_accuracies = [train_acc_rf, train_acc_xgb]
testing_accuracies = [test_acc_rf, test_acc_xgb]

plt.figure(figsize=(8, 6))
bar_width = 0.35
index = np.arange(len(model_names))

# Plot training accuracies
plt.bar(index, training_accuracies, bar_width, label='Training Accuracy', color='b')

# Plot testing accuracies
plt.bar(index + bar_width, testing_accuracies, bar_width, label='Testing Accuracy', color='g')

plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy Comparison')
plt.xticks(index + bar_width / 2, model_names)
plt.legend()

plt.tight_layout()
plt.show()

# Store accuracies for both models
train_acc_rf = grid_rf.score(X_train, y_train)
test_acc_rf = grid_rf.score(X_test, y_test)

train_acc_xgb = pipe_xgb.score(X_train, y_train)
test_acc_xgb = pipe_xgb.score(X_test, y_test)

# Print the accuracies
print(f"Random Forest - Training Accuracy: {train_acc_rf:.2f}")
print(f"Random Forest - Testing Accuracy: {test_acc_rf:.2f}")
print(f"XGBoost - Training Accuracy: {train_acc_xgb:.2f}")
print(f"XGBoost - Testing Accuracy: {test_acc_xgb:.2f}")

# Combine accuracies into arrays for plotting
model_names = ['Random Forest', 'XGBoost']
training_accuracies = [train_acc_rf, train_acc_xgb]
testing_accuracies = [test_acc_rf, test_acc_xgb]

# Plot Training and Testing Accuracy for each model
plt.figure(figsize=(8, 6))
bar_width = 0.35
index = np.arange(len(model_names))

# Plot training accuracies
plt.bar(index, training_accuracies, bar_width, label='Training Accuracy', color='b')

# Plot testing accuracies
plt.bar(index + bar_width, testing_accuracies, bar_width, label='Testing Accuracy', color='g')

plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy Comparison')
plt.xticks(index + bar_width / 2, model_names)
plt.legend()

plt.tight_layout()
plt.show()



from google.colab import drive
drive.mount('/content/drive')







import matplotlib.pyplot as plt

# Accuracy data based on preferences
models = ['Random Forest (Pref 1)', 'XGBoost (Pref 1)', 'Random Forest (Pref 2)', 'XGBoost (Pref 2)']
training_acc = [98.50, 98.40, 99.00, 99.10]
testing_acc = [97.80, 98.03, 98.03, 98.03]

# Plotting the accuracies
plt.figure(figsize=(10, 6))

# Training accuracy
plt.plot(models, training_acc, marker='o', label='Training Accuracy', color='blue', linestyle='dashed', linewidth=2, markersize=8)

# Testing accuracy
plt.plot(models, testing_acc, marker='o', label='Testing Accuracy', color='green', linestyle='dashed', linewidth=2, markersize=8)

# Adding plot details
plt.title('Training vs Testing Accuracy for Random Forest and XGBoost Models', fontsize=14)
plt.xlabel('Models', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.legend(loc='lower right')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Example training and testing accuracy values
train_acc_rf = 0.98  # Random Forest training accuracy
test_acc_rf = 0.95   # Random Forest testing accuracy
train_acc_xgb = 0.97  # XGBoost training accuracy
test_acc_xgb = 0.96   # XGBoost testing accuracy

# Models and their accuracies
models = ['Random Forest', 'XGBoost']
training_acc = [train_acc_rf * 100, train_acc_xgb * 100]
testing_acc = [test_acc_rf * 100, test_acc_xgb * 100]

# Plotting the accuracies
plt.figure(figsize=(10, 6))

# Set positions of bars on the x-axis
x = range(len(models))

# Bar width
width = 0.35

# Create bars for training accuracy
plt.bar(x, training_acc, width=width, label='Training Accuracy', color='blue', alpha=0.7)

# Create bars for testing accuracy, offsetting them by width
plt.bar([p + width for p in x], testing_acc, width=width, label='Testing Accuracy', color='green', alpha=0.7)

# Adding plot details
plt.title('Training vs Testing Accuracy for Random Forest and XGBoost Models', fontsize=16)
plt.ylabel('Accuracy (%)', fontsize=14)
plt.xticks([p + width / 2 for p in x], models)  # Center x-ticks
plt.legend(loc='lower right')
plt.ylim(0, 100)
plt.grid(axis='y')

# Display the plot
plt.tight_layout()
plt.show()

import pandas as pd

# Example accuracy values for different models
data = {
    'Model': ['Random Forest (MFCC)', 'XGBoost (MFCC)', 'Decision Tree (ZCR)',
              'KNN (RMS)', 'SVM (TSI)', 'Bagging (ZCR+RMS)', 'Logistic Regression (MFCC+RMS)'],
    'Training Accuracy (%)': [98.03, 97.50, 94.00, 92.50, 95.00, 93.50, 90.00],
    'Testing Accuracy (%)': [95.00, 96.00, 90.00, 88.00, 91.00, 89.00, 87.00]
}

# Create a DataFrame
accuracy_df = pd.DataFrame(data)

# Display the DataFrame
print(accuracy_df)

# Optional: Save to a CSV file
accuracy_df.to_csv('model_accuracy_comparison.csv', index=False)

import matplotlib.pyplot as plt

# Accuracy data for Random Forest
models_rf = ['Random Forest (Pref 1)', 'Random Forest (Pref 2)']
training_acc_rf = [98.50, 99.00]  # Replace with your actual training accuracies
testing_acc_rf = [97.80, 98.03]    # Replace with your actual testing accuracies

# Plotting the accuracies
plt.figure(figsize=(10, 6))

# Training accuracy
plt.plot(models_rf, training_acc_rf, marker='o', label='Training Accuracy', color='blue', linestyle='dashed', linewidth=2, markersize=8)

# Testing accuracy
plt.plot(models_rf, testing_acc_rf, marker='o', label='Testing Accuracy', color='green', linestyle='dashed', linewidth=2, markersize=8)

# Adding plot details
plt.title('Training vs Testing Accuracy for Random Forest Models', fontsize=14)
plt.xlabel('Models', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.legend(loc='lower right')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Example loss data (replace with your actual values)
models_rf = ['Random Forest (Pref 1)', 'Random Forest (Pref 2)']
training_loss_rf = [0.05, 0.03]  # Replace with your actual training losses
testing_loss_rf = [0.07, 0.05]    # Replace with your actual testing losses

# Plotting the losses
plt.figure(figsize=(10, 6))

# Training loss
plt.plot(models_rf, training_loss_rf, marker='o', label='Training Loss', color='blue', linestyle='dashed', linewidth=2, markersize=8)

# Testing loss
plt.plot(models_rf, testing_loss_rf, marker='o', label='Testing Loss', color='red', linestyle='dashed', linewidth=2, markersize=8)

# Adding plot details
plt.title('Training vs Testing Loss for Random Forest Models', fontsize=14)
plt.xlabel('Models', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(loc='upper right')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Accuracy data for Random Forest with one preference
model_rf = 'Random Forest (Pref 1)'
training_accuracy_rf = 98.50  # Replace with your actual training accuracy
testing_accuracy_rf = 97.80    # Replace with your actual testing accuracy

# Plotting the accuracies
plt.figure(figsize=(10, 6))

# Bar plot for training accuracy
plt.bar(model_rf, training_accuracy_rf, width=0.4, label='Training Accuracy', color='blue', align='center')

# Bar plot for testing accuracy
plt.bar(model_rf, testing_accuracy_rf, width=0.4, label='Testing Accuracy', color='green', align='edge')

# Adding plot details
plt.title('Training vs Testing Accuracy for Random Forest Model (Pref 1)', fontsize=14)
plt.xlabel('Models', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.legend(loc='upper right')
plt.ylim(0, 100)  # Set y-axis limit for better visibility
plt.grid(axis='y')
plt.tight_layout()

# Display the plot
plt.show()

# Example reference
reference = "Smith, J., & Doe, A. (2023). Advanced Machine Learning Techniques for Infant Cry Classification. Journal of Audio Processing, 12(3), 45-67."
print("Reference:", reference)

import matplotlib.pyplot as plt

# Example loss data for Random Forest with one preference
model_rf = 'Random Forest '
training_loss_rf = 0.05  # Replace with your actual training loss
testing_loss_rf = 0.07    # Replace with your actual testing loss

# Plotting the losses
plt.figure(figsize=(10, 6))

# Training loss
plt.bar(model_rf, training_loss_rf, width=0.4, label='Training Loss', color='blue', align='center')

# Testing loss
plt.bar(model_rf, testing_loss_rf, width=0.4, label='Testing Loss', color='red', align='edge')

# Adding plot details
plt.title('Training vs Testing Loss for Random Forest Model ', fontsize=14)
plt.xlabel('Models', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(loc='upper right')
plt.grid(axis='y')
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Accuracy data for Random Forest
model_rf = 'Random Forest '
training_accuracy_rf = 98.50  # Replace with your actual training accuracy
testing_accuracy_rf = 95.08    # Replace with your actual testing accuracy

# Data for plotting
accuracies = [training_accuracy_rf, testing_accuracy_rf]
labels = ['Training Accuracy', 'Testing Accuracy']

# Create a bar plot
plt.figure(figsize=(8, 5))
x_pos = np.arange(len(labels))

plt.bar(x_pos, accuracies, color=['blue', 'green'], alpha=0.7)
plt.xticks(x_pos, labels)

# Adding plot details
plt.title('Training vs Testing Accuracy for Random Forest Model ', fontsize=14)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.ylim(0, 100)  # Set y-axis limit for better visibility
plt.grid(axis='y')

# Display the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Sample data for demonstration (replace with actual model training/testing accuracy data)
epochs = np.arange(1, 11)  # Replace with actual epochs or iterations

# Add Naive Bayes data (replace with actual data)
nb_training_acc = [80.0, 82.5, 84.0, 85.5, 87.0, 88.0, 89.5, 90.0, 91.0, 91.5]
nb_testing_acc = [78.0, 80.0, 81.5, 83.0, 84.5, 86.0, 87.5, 88.0, 89.0, 90.0]

xgb_training_acc = [88.5, 90.0, 91.2, 92.8, 94.0, 95.5, 96.2, 96.8, 97.4, 98.0]
xgb_testing_acc = [87.5, 89.0, 90.2, 91.8, 92.5, 93.7, 94.8, 95.5, 96.2, 97.1]

svm_training_acc = [85.0, 87.0, 88.5, 89.8, 91.0, 92.0, 93.2, 94.0, 95.5, 96.5]
svm_testing_acc = [84.0, 86.0, 87.0, 88.5, 90.0, 91.0, 92.2, 93.5, 94.0, 95.0]

knn_training_acc = [82.0, 84.5, 86.0, 87.5, 88.8, 90.0, 91.5, 92.5, 93.5, 94.0]
knn_testing_acc = [81.0, 83.0, 85.0, 86.5, 88.0, 89.2, 90.5, 91.0, 92.0, 93.0]

# Add Logistic Regression data
lr_training_acc = [87.5, 89.0, 90.5, 91.8, 93.0, 94.0, 95.0, 96.0, 96.5, 97.0]
lr_testing_acc = [86.0, 87.5, 89.0, 90.5, 91.8, 92.5, 93.5, 94.2, 95.0, 95.8]

# Add Decision Tree data
dt_training_acc = [84.0, 85.5, 87.0, 88.5, 90.0, 91.2, 92.5, 93.5, 94.5, 95.0]
dt_testing_acc = [83.0, 84.5, 86.0, 87.5, 89.0, 90.0, 91.2, 92.0, 93.0, 93.5]



# Create individual plots for each model
plt.figure(figsize=(12, 12))

# Plot for Naive Bayes
plt.subplot(4, 2, 1)
plt.plot(epochs, nb_training_acc, label='Training Accuracy', marker='p', linestyle='--', color='blue')
plt.plot(epochs, nb_testing_acc, label='Testing Accuracy', marker='p', color='green')
plt.title('Naive Bayes')
plt.xlabel('Epochs/Iterations')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)

# Plot for XGBoost
plt.subplot(4, 2, 2)
plt.plot(epochs, xgb_training_acc, label='Training Accuracy', marker='^', linestyle='--', color='blue')
plt.plot(epochs, xgb_testing_acc, label='Testing Accuracy', marker='^', color='green')
plt.title('XGBoost')
plt.xlabel('Epochs/Iterations')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)

# Plot for SVM
plt.subplot(4, 2, 3)
plt.plot(epochs, svm_training_acc, label='Training Accuracy', marker='s', linestyle='--', color='blue')
plt.plot(epochs, svm_testing_acc, label='Testing Accuracy', marker='s', color='green')
plt.title('SVM')
plt.xlabel('Epochs/Iterations')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)

# Plot for KNN
plt.subplot(4, 2, 4)
plt.plot(epochs, knn_training_acc, label='Training Accuracy', marker='d', linestyle='--', color='blue')
plt.plot(epochs, knn_testing_acc, label='Testing Accuracy', marker='d', color='green')
plt.title('KNN')
plt.xlabel('Epochs/Iterations')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)

# Plot for Logistic Regression
plt.subplot(4, 2, 5)
plt.plot(epochs, lr_training_acc, label='Training Accuracy', marker='*', linestyle='--', color='blue')
plt.plot(epochs, lr_testing_acc, label='Testing Accuracy', marker='*', color='green')
plt.title('Logistic Regression')
plt.xlabel('Epochs/Iterations')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)

# Plot for Decision Tree
plt.subplot(4, 2, 6)
plt.plot(epochs, dt_training_acc, label='Training Accuracy', marker='x', linestyle='--', color='blue')
plt.plot(epochs, dt_testing_acc, label='Testing Accuracy', marker='x', color='green')
plt.title('Decision Tree')
plt.xlabel('Epochs/Iterations')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)



# Adjust layout
plt.tight_layout()

# Display the plots
plt.show()

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix

# Directory path to dataset (replace with the actual path to your dataset folder)
dataset_dir = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data'

# Function to load all audio files and extract their corresponding labels
def load_dataset(directory):
    features = []
    labels = []
    for class_label in os.listdir(directory):
        class_dir = os.path.join(directory, class_label)
        if os.path.isdir(class_dir):
            for file_name in os.listdir(class_dir):
                file_path = os.path.join(class_dir, file_name)
                if file_path.endswith('.wav'):
                    feat = extract_features(file_path)
                    features.append(feat)
                    labels.append(class_label)
    return np.array(features), np.array(labels)

# Feature extraction function
def extract_features(file_path, sr=22050):
    y, sr = librosa.load(file_path, sr=sr, duration=5.0)
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))
    rms = np.mean(librosa.feature.rms(y=y))
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)
    return [zcr, rms] + mfccs.tolist()

# Load dataset
X, y = load_dataset(dataset_dir)

# Convert labels to numerical values
label_map = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_map[label] for label in y])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()

# Initialize Naive Bayes model
nb = GaussianNB()

# Create pipeline
pipe_nb = Pipeline([('scaler', scaler), ('nb', nb)])

# Fit the Naive Bayes model
pipe_nb.fit(X_train, y_train)

# Predictions
y_pred_nb = pipe_nb.predict(X_test)

# Evaluation
train_acc_nb = pipe_nb.score(X_train, y_train)
test_acc_nb = pipe_nb.score(X_test, y_test)
print(f"Naive Bayes - Training Accuracy: {train_acc_nb:.2f}")
print(f"Naive Bayes - Testing Accuracy: {test_acc_nb:.2f}")

# Detailed Classification Report
print("\nNaive Bayes Classification Report:")
print(classification_report(y_test, y_pred_nb))

# Confusion Matrix
cm_nb = confusion_matrix(y_test, y_pred_nb)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())
plt.title('Naive Bayes Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Directory path to dataset (replace with the actual path to your dataset folder)
dataset_dir = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data'

# Function to load all audio files and extract their corresponding labels
def load_dataset(directory):
    features = []
    labels = []
    for class_label in os.listdir(directory):
        class_dir = os.path.join(directory, class_label)
        if os.path.isdir(class_dir):
            for file_name in os.listdir(class_dir):
                file_path = os.path.join(class_dir, file_name)
                if file_path.endswith('.wav'):
                    feat = extract_features(file_path)
                    features.append(feat)
                    labels.append(class_label)
    return np.array(features), np.array(labels)

# Feature extraction function
def extract_features(file_path, sr=22050):
    y, sr = librosa.load(file_path, sr=sr, duration=5.0)
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))
    rms = np.mean(librosa.feature.rms(y=y))
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)
    return [zcr, rms] + mfccs.tolist()

# Load dataset
X, y = load_dataset(dataset_dir)

# Convert labels to numerical values
label_map = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_map[label] for label in y])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()

# Initialize KNN model
knn = KNeighborsClassifier()

# KNN model and hyperparameter tuning
param_grid_knn = {
    'knn__n_neighbors': [3, 5, 7],  # Access KNN params through 'knn__param_name'
    'knn__weights': ['uniform', 'distance'],
}

# Create pipeline
pipe_knn = Pipeline([('scaler', scaler), ('knn', knn)])
grid_knn = GridSearchCV(pipe_knn, param_grid_knn, cv=5, verbose=1, n_jobs=-1)
grid_knn.fit(X_train, y_train)

# Predictions
y_pred_knn = grid_knn.predict(X_test)

# Evaluation
train_acc_knn = grid_knn.score(X_train, y_train)
test_acc_knn = grid_knn.score(X_test, y_test)
print(f"KNN - Best Parameters: {grid_knn.best_params_}")
print(f"KNN - Training Accuracy: {train_acc_knn:.2f}")
print(f"KNN - Testing Accuracy: {test_acc_knn:.2f}")

# Detailed Classification Report
print("\nKNN Classification Report:")
print(classification_report(y_test, y_pred_knn))

# Confusion Matrix
cm_knn = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())
plt.title('KNN Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Directory path to dataset (replace with the actual path to your dataset folder)
dataset_dir = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data'

# Function to load all audio files and extract their corresponding labels
def load_dataset(directory):
    features = []
    labels = []
    for class_label in os.listdir(directory):
        class_dir = os.path.join(directory, class_label)
        if os.path.isdir(class_dir):
            for file_name in os.listdir(class_dir):
                file_path = os.path.join(class_dir, file_name)
                if file_path.endswith('.wav'):
                    feat = extract_features(file_path)
                    features.append(feat)
                    labels.append(class_label)
    return np.array(features), np.array(labels)

# Feature extraction function
def extract_features(file_path, sr=22050):
    y, sr = librosa.load(file_path, sr=sr, duration=5.0)
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))
    rms = np.mean(librosa.feature.rms(y=y))
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)
    return [zcr, rms] + mfccs.tolist()

# Load dataset
X, y = load_dataset(dataset_dir)

# Convert labels to numerical values
label_map = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_map[label] for label in y])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()

# Initialize XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Hyperparameter tuning with GridSearchCV for XGBoost
param_grid_xgb = {
    'xgb__n_estimators': [100, 200],
    'xgb__max_depth': [3, 5, 7],
    'xgb__learning_rate': [0.01, 0.1],
    'xgb__subsample': [0.8, 1.0]
}

# Create pipeline
pipe_xgb = Pipeline([('scaler', scaler), ('xgb', xgb)])
grid_xgb = GridSearchCV(pipe_xgb, param_grid_xgb, cv=5, verbose=1, n_jobs=-1)
grid_xgb.fit(X_train, y_train)

# Predictions
y_pred_xgb = grid_xgb.predict(X_test)

# Evaluation
train_acc_xgb = grid_xgb.score(X_train, y_train)
test_acc_xgb = grid_xgb.score(X_test, y_test)
print(f"XGBoost - Best Parameters: {grid_xgb.best_params_}")
print(f"XGBoost - Training Accuracy: {train_acc_xgb:.2f}")
print(f"XGBoost - Testing Accuracy: {test_acc_xgb:.2f}")

# Detailed Classification Report
print("\nXGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

# Confusion Matrix
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())
plt.title('Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Directory path to dataset (replace with the actual path to your dataset folder)
dataset_dir = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data'

# Function to load all audio files and extract their corresponding labels
def load_dataset(directory):
    features = []
    labels = []
    for class_label in os.listdir(directory):
        class_dir = os.path.join(directory, class_label)
        if os.path.isdir(class_dir):
            for file_name in os.listdir(class_dir):
                file_path = os.path.join(class_dir, file_name)
                if file_path.endswith('.wav'):
                    feat = extract_features(file_path)
                    features.append(feat)
                    labels.append(class_label)
    return np.array(features), np.array(labels)

# Feature extraction function
def extract_features(file_path, sr=22050):
    y, sr = librosa.load(file_path, sr=sr, duration=5.0)
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))
    rms = np.mean(librosa.feature.rms(y=y))
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)
    return [zcr, rms] + mfccs.tolist()

# Load dataset
X, y = load_dataset(dataset_dir)

# Convert labels to numerical values
label_map = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_map[label] for label in y])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()

# Initialize Decision Tree model
dt = DecisionTreeClassifier(random_state=42)

# Hyperparameter tuning with GridSearchCV for Decision Tree
param_grid_dt = {
    'dt__max_depth': [None, 5, 10, 15],
    'dt__min_samples_split': [2, 5, 10],
    'dt__min_samples_leaf': [1, 2, 4],
}

# Create pipeline
pipe_dt = Pipeline([('scaler', scaler), ('dt', dt)])
grid_dt = GridSearchCV(pipe_dt, param_grid_dt, cv=5, verbose=1, n_jobs=-1)
grid_dt.fit(X_train, y_train)

# Predictions
y_pred_dt = grid_dt.predict(X_test)

# Evaluation
train_acc_dt = grid_dt.score(X_train, y_train)
test_acc_dt = grid_dt.score(X_test, y_test)
print(f"Decision Tree - Best Parameters: {grid_dt.best_params_}")
print(f"Decision Tree - Training Accuracy: {train_acc_dt:.2f}")
print(f"Decision Tree - Testing Accuracy: {test_acc_dt:.2f}")

# Detailed Classification Report
print("\nDecision Tree Classification Report:")
print(classification_report(y_test, y_pred_dt))

# Confusion Matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())
plt.title('Decision Tree Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Directory path to dataset (replace with the actual path to your dataset folder)
dataset_dir = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data'

# Function to load all audio files and extract their corresponding labels
def load_dataset(directory):
    features = []
    labels = []
    for class_label in os.listdir(directory):
        class_dir = os.path.join(directory, class_label)
        if os.path.isdir(class_dir):
            for file_name in os.listdir(class_dir):
                file_path = os.path.join(class_dir, file_name)
                if file_path.endswith('.wav'):
                    feat = extract_features(file_path)
                    features.append(feat)
                    labels.append(class_label)
    return np.array(features), np.array(labels)

# Feature extraction function
def extract_features(file_path, sr=22050):
    y, sr = librosa.load(file_path, sr=sr, duration=5.0)
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))
    rms = np.mean(librosa.feature.rms(y=y))
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)
    return [zcr, rms] + mfccs.tolist()

# Load dataset
X, y = load_dataset(dataset_dir)

# Convert labels to numerical values
label_map = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_map[label] for label in y])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()

# Initialize Logistic Regression model
lr = LogisticRegression(max_iter=1000, random_state=42)

# Hyperparameter tuning with GridSearchCV for Logistic Regression
param_grid_lr = {
    'lr__C': [0.01, 0.1, 1, 10],  # Regularization strength
    'lr__solver': ['lbfgs', 'liblinear']  # Solvers to try
}

# Create pipeline
pipe_lr = Pipeline([('scaler', scaler), ('lr', lr)])
grid_lr = GridSearchCV(pipe_lr, param_grid_lr, cv=5, verbose=1, n_jobs=-1)
grid_lr.fit(X_train, y_train)

# Predictions
y_pred_lr = grid_lr.predict(X_test)

# Evaluation
train_acc_lr = grid_lr.score(X_train, y_train)
test_acc_lr = grid_lr.score(X_test, y_test)
print(f"Logistic Regression - Best Parameters: {grid_lr.best_params_}")
print(f"Logistic Regression - Training Accuracy: {train_acc_lr:.2f}")
print(f"Logistic Regression - Testing Accuracy: {test_acc_lr:.2f}")

# Detailed Classification Report
print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, y_pred_lr))

# Confusion Matrix
cm_lr = confusion_matrix(y_test, y_pred_lr)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())
plt.title('Logistic Regression Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Best parameters after tuning
best_rf_params = grid_rf.best_params_
print("Best Parameters for Random Forest: ", best_rf_params)
{
    'rf__n_estimators': 200,
    'rf__max_depth': 20,
    'rf__min_samples_split': 5,
    'rf__min_samples_leaf': 2
}
# Testing accuracy
test_acc_rf = grid_rf.score(X_test, y_test)
print(f"Testing Accuracy for Random Forest: {test_acc_rf:.2f}")

# Confusion matrix for test set
y_pred_rf = grid_rf.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix for Random Forest: \n", cm_rf)
# Count the number of trees (estimators)
n_estimators_rf = len(grid_rf.best_estimator_.named_steps['rf'].estimators_)
print("Number of trees in the Random Forest: ", n_estimators_rf)

import unittest
class TestYourCode(unittest.TestCase):
    # Test methods will go here
    pass
class TestYourCode(unittest.TestCase):
    def test_extract_features(self):
        # Load a sample audio file
        file_path = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data/hungry/hungry_001.wav'  # Replace with an actual file path
        features, mel = extract_features(file_path)

        # Check the number of features extracted
        self.assertEqual(len(features), 22)

        # Check the type of features
        self.assertIsInstance(features, list)
import unittest
class TestYourCode(unittest.TestCase):
    # Test methods will go here
    pass
class TestYourCode(unittest.TestCase):
    def test_extract_features(self):
        # Load a sample audio file
        file_path = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data/hungry/hungry_001.wav'  # Replace with an actual file path
        features, mel = extract_features(file_path)

        # Check the number of features extracted
        self.assertEqual(len(features), 22)

        # Check the type of features
        self.assertIsInstance(features, list)

        # Add more assertions to check specific feature values if needed

import unittest
class TestYourCode(unittest.TestCase):
    # Test methods will go here
    pass
class TestYourCode(unittest.TestCase):
    def test_extract_features(self):
        # Load a sample audio file
        file_path = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data/hungry/hungry_001.wav'  # Replace with an actual file path
        features, mel = extract_features(file_path)

        # Check the number of features extracted
        self.assertEqual(len(features), 22)

        # Check the type of features
        self.assertIsInstance(features, list)
import unittest
class TestYourCode(unittest.TestCase):
    # Test methods will go here
    pass
class TestYourCode(unittest.TestCase):
    def test_extract_features(self):
        # Load a sample audio file
        file_path = '/content/drive/MyDrive/donateacry_corpus_cleaned_and_updated_data/hungry/hungry_001.wav'  # Replace with an actual file path
        features, mel = extract_features(file_path)

        # Check the number of features extracted
        self.assertEqual(len(features), 22)

        # Check the type of features
        self.assertIsInstance(features, list)

        # Add more assertions to check specific feature values if needed

if __name__ == '__main__':
    unittest.main(argv=['first-arg-is-ignored'], exit=False)